{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pgmpy.factors.discrete import TabularCPD\n",
    "from pgmpy.models import BayesianModel\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from pgmpy.inference import VariableElimination\n",
    "from tqdm import tqdm\n",
    "from pgmpy.readwrite import BIFWriter\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_data = pd.read_csv(\"../../dataset/15features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,columns in enumerate(feature_data.columns):\n",
    "    if columns != \"imagename\":\n",
    "        feature_data[str(columns)] = feature_data[str(columns)] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imagename</th>\n",
       "      <th>pen_pressure</th>\n",
       "      <th>letter_spacing</th>\n",
       "      <th>size</th>\n",
       "      <th>dimension</th>\n",
       "      <th>is_lowercase</th>\n",
       "      <th>is_continuous</th>\n",
       "      <th>slantness</th>\n",
       "      <th>tilt</th>\n",
       "      <th>entry_stroke_a</th>\n",
       "      <th>staff_of_a</th>\n",
       "      <th>formation_n</th>\n",
       "      <th>staff_of_d</th>\n",
       "      <th>exit_stroke_d</th>\n",
       "      <th>word_formation</th>\n",
       "      <th>constancy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0968c_num1.png</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0809c_num2.png</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0237b_num6.png</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0069b_num2.png</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0966c_num4.png</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        imagename  pen_pressure  letter_spacing  size  dimension  \\\n",
       "0  0968c_num1.png             1               1     1          0   \n",
       "1  0809c_num2.png             1               1     1          1   \n",
       "2  0237b_num6.png             1               1     1          1   \n",
       "3  0069b_num2.png             1               1     1          0   \n",
       "4  0966c_num4.png             1               1     1          1   \n",
       "\n",
       "   is_lowercase  is_continuous  slantness  tilt  entry_stroke_a  staff_of_a  \\\n",
       "0             1              1          2     1               0           1   \n",
       "1             1              1          2     0               0           1   \n",
       "2             1              1          1     1               0           1   \n",
       "3             1              1          0     0               0           0   \n",
       "4             1              1          1     0               0           1   \n",
       "\n",
       "   formation_n  staff_of_d  exit_stroke_d  word_formation  constancy  \n",
       "0            1           2              1               1          0  \n",
       "1            1           2              0               1          1  \n",
       "2            1           1              1               1          1  \n",
       "3            0           0              0               0          0  \n",
       "4            1           1              1               1          1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>left</th>\n",
       "      <th>right</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1469b_num3.png</td>\n",
       "      <td>1469b_num2.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1469b_num3.png</td>\n",
       "      <td>1469c_num2.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1469b_num3.png</td>\n",
       "      <td>1469a_num1.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1469b_num3.png</td>\n",
       "      <td>1469a_num3.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1469b_num3.png</td>\n",
       "      <td>1469c_num1.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0            left           right  label\n",
       "0           0  1469b_num3.png  1469b_num2.png      1\n",
       "1           1  1469b_num3.png  1469c_num2.png      1\n",
       "2           2  1469b_num3.png  1469a_num1.png      1\n",
       "3           3  1469b_num3.png  1469a_num3.png      1\n",
       "4           4  1469b_num3.png  1469c_num1.png      1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seen_train = pd.read_csv(\"../../dataset/unseen-dataset/dataset_seen_training_siamese.csv\")\n",
    "val_data = pd.read_csv(\"../../dataset/unseen-dataset/dataset_seen_validation_siamese.csv\")\n",
    "val_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = pd.merge(seen_train,feature_data.add_suffix('1'),left_on=\"left\",right_on=\"imagename1\",how=\"inner\")\n",
    "trainData = pd.merge(trainData,feature_data.add_suffix('2'),left_on=\"right\",right_on=\"imagename2\",how=\"inner\")\n",
    "trainData = trainData.drop([\"Unnamed: 0\",\"imagename1\",\"imagename2\"],axis=1)\n",
    "val_data = pd.merge(val_data,feature_data.add_suffix('1'),left_on=\"left\",right_on=\"imagename1\",how=\"inner\")\n",
    "val_data = pd.merge(val_data,feature_data.add_suffix('2'),left_on=\"right\",right_on=\"imagename2\",how=\"inner\")\n",
    "val_data = val_data.drop([\"Unnamed: 0\",\"imagename1\",\"imagename2\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>left</th>\n",
       "      <th>right</th>\n",
       "      <th>label</th>\n",
       "      <th>pen_pressure1</th>\n",
       "      <th>letter_spacing1</th>\n",
       "      <th>size1</th>\n",
       "      <th>dimension1</th>\n",
       "      <th>is_lowercase1</th>\n",
       "      <th>is_continuous1</th>\n",
       "      <th>slantness1</th>\n",
       "      <th>...</th>\n",
       "      <th>is_continuous2</th>\n",
       "      <th>slantness2</th>\n",
       "      <th>tilt2</th>\n",
       "      <th>entry_stroke_a2</th>\n",
       "      <th>staff_of_a2</th>\n",
       "      <th>formation_n2</th>\n",
       "      <th>staff_of_d2</th>\n",
       "      <th>exit_stroke_d2</th>\n",
       "      <th>word_formation2</th>\n",
       "      <th>constancy2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1469b_num3.png</td>\n",
       "      <td>1469b_num2.png</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1453b_num4.png</td>\n",
       "      <td>1469b_num2.png</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1513b_num3.png</td>\n",
       "      <td>1469b_num2.png</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1515c_num2.png</td>\n",
       "      <td>1469b_num2.png</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1517b_num1.png</td>\n",
       "      <td>1469b_num2.png</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             left           right  label  pen_pressure1  letter_spacing1  \\\n",
       "0  1469b_num3.png  1469b_num2.png      1              1                1   \n",
       "1  1453b_num4.png  1469b_num2.png      0              0                1   \n",
       "2  1513b_num3.png  1469b_num2.png      0              0                1   \n",
       "3  1515c_num2.png  1469b_num2.png      0              1                2   \n",
       "4  1517b_num1.png  1469b_num2.png      0              1                2   \n",
       "\n",
       "   size1  dimension1  is_lowercase1  is_continuous1  slantness1  ...  \\\n",
       "0      1           1              1               1           1  ...   \n",
       "1      1           1              1               0           0  ...   \n",
       "2      1           0              1               0           1  ...   \n",
       "3      1           1              1               1           3  ...   \n",
       "4      0           0              1               1           2  ...   \n",
       "\n",
       "   is_continuous2  slantness2  tilt2  entry_stroke_a2  staff_of_a2  \\\n",
       "0               0           0      0                0            0   \n",
       "1               0           0      0                0            0   \n",
       "2               0           0      0                0            0   \n",
       "3               0           0      0                0            0   \n",
       "4               0           0      0                0            0   \n",
       "\n",
       "   formation_n2  staff_of_d2  exit_stroke_d2  word_formation2  constancy2  \n",
       "0             1            1               1                1           1  \n",
       "1             1            1               1                1           1  \n",
       "2             1            1               1                1           1  \n",
       "3             1            1               1                1           1  \n",
       "4             1            1               1                1           1  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_model = BayesianModel([('pen_pressure1','is_pen_pressure_sim'),\n",
    "                                ('pen_pressure2','is_pen_pressure_sim'),\n",
    "                                ('slantness1','is_slantness_sim'),\n",
    "                                ('slantness2','is_slantness_sim'),\n",
    "                                ('tilt1','is_tilt_sim'),\n",
    "                                ('tilt2','is_tilt_sim'),\n",
    "                                ('is_slantness_sim','is_tilt_sim'),\n",
    "                                ('staff_of_a1','is_staff_of_a_sim'),\n",
    "                                ('staff_of_a2','is_staff_of_a_sim'),\n",
    "                                ('staff_of_d1','is_staff_of_d_sim'),\n",
    "                                ('staff_of_d2','is_staff_of_d_sim'),\n",
    "                                ('is_staff_of_a_sim','is_staff_of_d_sim'),\n",
    "                                ('entry_stroke_a1','entry_stroke_a_sim'),\n",
    "                                ('entry_stroke_a2','entry_stroke_a_sim'),\n",
    "                                ('exit_stroke_d1','is_exit_stroke_d_sim'),\n",
    "                                ('exit_stroke_d2','is_exit_stroke_d_sim'),\n",
    "                                ('entry_stroke_a_sim','is_exit_stroke_d_sim'),\n",
    "                                ('is_lowercase1','is_lowercase_sim'),\n",
    "                                ('is_lowercase2','is_lowercase_sim'),\n",
    "                                ('is_continuous1','is_continuous_sim'),\n",
    "                                ('is_continuous2','is_continuous_sim'),\n",
    "                                ('is_lowercase_sim','is_continuous_sim'),\n",
    "                                ('dimension1','dimension_sim'),\n",
    "                                ('dimension2','dimension_sim'),\n",
    "                                ('letter_spacing1','letter_spacing_sim'),\n",
    "                                ('letter_spacing2','letter_spacing_sim'),\n",
    "                                ('size1','size_sim'),\n",
    "                                ('size2','size_sim'),\n",
    "                                ('dimension_sim','size_sim'),\n",
    "                                ('letter_spacing_sim','size_sim'),\n",
    "                                ('constancy1','constancy_sim'),\n",
    "                                ('constancy2','constancy_sim'),\n",
    "                                ('size_sim','constancy_sim'),\n",
    "                                ('word_formation1','word_formation_sim'),\n",
    "                                ('word_formation2','word_formation_sim'),\n",
    "                                ('constancy_sim','word_formation_sim'),\n",
    "                                ('formation_n1','formation_n_sim'),\n",
    "                                ('formation_n2','formation_n_sim'),\n",
    "                                ('word_formation_sim','formation_n_sim')\n",
    "                               ])\n",
    "\n",
    "cpd_pen_pressure1 = TabularCPD('pen_pressure1',2,[[0.5],\n",
    "                                                [0.5]],\n",
    "                                                evidence=[], evidence_card=[])\n",
    "cpd_pen_pressure2 = TabularCPD('pen_pressure2',2,[[0.5],\n",
    "                                                [0.5]],\n",
    "                                                evidence=[], evidence_card=[])\n",
    "cpd_is_pen_pressure_sim = TabularCPD('is_pen_pressure_sim',2,[[0.1,0.9,0.9,0.1],\n",
    "                                                            [0.9,0.1,0.1,0.9]],\n",
    "                                                            evidence=['pen_pressure1','pen_pressure2'], \n",
    "                                                            evidence_card=[2,2])\n",
    "cpd_slantness1 = TabularCPD('slantness1',4,[[0.25],[0.25],[0.25],[0.25]],\n",
    "                                                evidence=[], evidence_card=[])\n",
    "cpd_slantness2 = TabularCPD('slantness2',4,[[0.25],[0.25],[0.25],[0.25]],\n",
    "                                                evidence=[], evidence_card=[])\n",
    "cpd_is_slantness_sim = TabularCPD('is_slantness_sim',2,[[0.1,0.2,0.3,0.4,0.2,0.1,0.3,0.4,0.3,0.2,0.1,0.4,0.4,0.3,0.2,0.1],\n",
    "                                                            [0.9,0.8,0.7,0.6,0.8,0.9,0.7,0.6,0.7,0.8,0.9,0.6,0.6,0.7,0.8,0.9]],\n",
    "                                                            evidence=['slantness1','slantness2'], \n",
    "                                                            evidence_card=[4,4])\n",
    "cpd_tilt1 = TabularCPD('tilt1',2,[[0.5],\n",
    "                                                [0.5]],\n",
    "                                                evidence=[], evidence_card=[])\n",
    "cpd_tilt2 = TabularCPD('tilt2',2,[[0.5],\n",
    "                                                [0.5]],\n",
    "                                                evidence=[], evidence_card=[])\n",
    "cpd_is_tilt_sim = TabularCPD('is_tilt_sim',2,[[0.4,0.1,0.9,0.6,0.9,0.6,0.4,0.1],\n",
    "                                                            [0.6,0.9,0.1,0.4,0.1,0.4,0.6,0.9]],\n",
    "                                                            evidence=['tilt1','tilt2','is_slantness_sim'], \n",
    "                                                            evidence_card=[2,2,2])\n",
    "cpd_staff_of_a1 = TabularCPD('staff_of_a1',4,[[0.25],[0.25],[0.25],[0.25]],\n",
    "                                                evidence=[], evidence_card=[])\n",
    "cpd_staff_of_a2 = TabularCPD('staff_of_a2',4,[[0.25],[0.25],[0.25],[0.25]],\n",
    "                                                evidence=[], evidence_card=[])\n",
    "cpd_is_staff_of_a_sim = TabularCPD('is_staff_of_a_sim',2,[[0.1,0.2,0.3,0.4,0.2,0.1,0.3,0.4,0.3,0.2,0.1,0.4,0.4,0.3,0.2,0.1],\n",
    "                                                            [0.9,0.8,0.7,0.6,0.8,0.9,0.7,0.6,0.7,0.8,0.9,0.6,0.6,0.7,0.8,0.9]],\n",
    "                                                            evidence=['staff_of_a1','staff_of_a2'], \n",
    "                                                            evidence_card=[4,4])\n",
    "cpd_staff_of_d1 = TabularCPD('staff_of_d1',3,[[0.33],\n",
    "                                    [0.34],[0.33]],\n",
    "                                    evidence=[], evidence_card=[])\n",
    "cpd_staff_of_d2 = TabularCPD('staff_of_d2',3,[[0.33],\n",
    "                                    [0.34],[0.33]],\n",
    "                                    evidence=[], evidence_card=[])\n",
    "cpd_is_staff_of_d_sim = TabularCPD('is_staff_of_d_sim',2,[[0.4,0.1,0.9,0.6,0.9,0.6,0.1,0.6,0.4,0.1,0.9,0.6,0.9,0.6,0.9,0.6,0.4,0.9],\n",
    "                                              [0.6,0.9,0.1,0.4,0.1,0.4,0.9,0.4,0.6,0.9,0.1,0.4,0.1,0.4,0.1,0.4,0.6,0.1]],\n",
    "                             evidence=['staff_of_d1','staff_of_d2','is_staff_of_a_sim'], \n",
    "                             evidence_card=[3,3,2])\n",
    "cpd_exit_stroke_d1 = TabularCPD('exit_stroke_d1',4,[[0.25],[0.25],[0.25],[0.25]],\n",
    "                                                evidence=[], evidence_card=[])\n",
    "cpd_exit_stroke_d2 = TabularCPD('exit_stroke_d2',4,[[0.25],[0.25],[0.25],[0.25]],\n",
    "                                                evidence=[], evidence_card=[])\n",
    "cpd_is_exit_stroke_d_sim = TabularCPD('is_exit_stroke_d_sim',2,[[0.9,0.1,0.9,0.6,0.9,0.6,0.9,0.6,0.9,0.6,0.4,0.1,0.9,0.6,0.9,0.6,0.9,0.6,0.9,0.6,0.4,0.1,0.9,0.6,0.9,0.6,0.9,0.6,0.9,0.6,0.4,0.1],\n",
    "                                                            [0.1,0.9,0.1,0.4,0.1,0.4,0.1,0.4,0.1,0.4,0.6,0.9,0.1,0.4,0.1,0.4,0.1,0.4,0.1,0.4,0.6,0.9,0.1,0.4,0.1,0.4,0.1,0.4,0.1,0.4,0.6,0.9]],\n",
    "                                                            evidence=['exit_stroke_d1','exit_stroke_d2','entry_stroke_a_sim'], \n",
    "                                                            evidence_card=[4,4,2])\n",
    "\n",
    "cpd_is_lowercase1 = TabularCPD('is_lowercase1',2,[[0.5],\n",
    "                                                [0.5]],\n",
    "                                                evidence=[], evidence_card=[])\n",
    "cpd_is_lowercase2 = TabularCPD('is_lowercase2',2,[[0.5],\n",
    "                                                [0.5]],\n",
    "                                                evidence=[], evidence_card=[])\n",
    "cpd_is_continuous1 = TabularCPD('is_continuous1',2,[[0.5],\n",
    "                                                [0.5]],\n",
    "                                                evidence=[], evidence_card=[])\n",
    "cpd_is_continuous2 = TabularCPD('is_continuous2',2,[[0.5],\n",
    "                                                [0.5]],\n",
    "                                                evidence=[], evidence_card=[])\n",
    "cpd_dimension1 = TabularCPD('dimension1',3,[[0.33],\n",
    "                                    [0.34],[0.33]],\n",
    "                                                evidence=[], evidence_card=[])\n",
    "cpd_dimension2 = TabularCPD('dimension2',3,[[0.33],\n",
    "                                    [0.34],[0.33]],\n",
    "                                                evidence=[], evidence_card=[])\n",
    "cpd_letter_spacing1 = TabularCPD('letter_spacing1',3,[[0.33],\n",
    "                                    [0.34],[0.33]],\n",
    "                                    evidence=[], evidence_card=[])\n",
    "cpd_letter_spacing2 = TabularCPD('letter_spacing2',3,[[0.33],\n",
    "                                    [0.34],[0.33]],\n",
    "                                    evidence=[], evidence_card=[])\n",
    "cpd_size1 = TabularCPD('size1',3,[[0.33],\n",
    "                                    [0.34],[0.33]],\n",
    "                                    evidence=[], evidence_card=[])\n",
    "cpd_size2 = TabularCPD('size2',3,[[0.33],\n",
    "                                    [0.34],[0.33]],\n",
    "                                    evidence=[], evidence_card=[])\n",
    "cpd_constancy1 = TabularCPD('constancy1',2,[[0.5],\n",
    "                                    [0.5]],\n",
    "                                    evidence=[], evidence_card=[])\n",
    "cpd_constancy2 = TabularCPD('constancy2',2,[[0.5],\n",
    "                                    [0.5]],\n",
    "                                    evidence=[], evidence_card=[])\n",
    "cpd_word_formation1 = TabularCPD('word_formation1',2,[[0.5],\n",
    "                                    [0.5]],\n",
    "                                    evidence=[], evidence_card=[])\n",
    "cpd_word_formation2 = TabularCPD('word_formation2',2,[[0.5],\n",
    "                                    [0.5]],\n",
    "                                    evidence=[], evidence_card=[])\n",
    "cpd_formation_n1 = TabularCPD('formation_n1',2,[[0.5],\n",
    "                                    [0.5]],\n",
    "                                    evidence=[], evidence_card=[])\n",
    "cpd_formation_n2 = TabularCPD('formation_n2',2,[[0.5],\n",
    "                                    [0.5]],\n",
    "                                    evidence=[], evidence_card=[])\n",
    "cpd_entry_stroke_a1 = TabularCPD('entry_stroke_a1',2,[[0.5],\n",
    "                                    [0.5]],\n",
    "                                    evidence=[], evidence_card=[])\n",
    "cpd_entry_stroke_a2 = TabularCPD('entry_stroke_a2',2,[[0.5],\n",
    "                                    [0.5]],\n",
    "                                    evidence=[], evidence_card=[])\n",
    "cpd_is_lowercase_sim = TabularCPD('is_lowercase_sim',2,[[0.1,0.9,0.9,0.1],\n",
    "                                                            [0.9,0.1,0.1,0.9]],\n",
    "                                                            evidence=['is_lowercase1','is_lowercase2'], \n",
    "                                                            evidence_card=[2,2])\n",
    "cpd_is_continuous_sim = TabularCPD('is_continuous_sim',2,[[0.9,0.1,0.9,0.6,0.9,0.6,0.9,0.1],\n",
    "                                                            [0.1,0.9,0.1,0.4,0.1,0.4,0.1,0.9]],\n",
    "                                                            evidence=['is_continuous1','is_continuous2','is_lowercase_sim'], \n",
    "                                                            evidence_card=[2,2,2])\n",
    "cpd_dimension_sim = TabularCPD('dimension_sim',2,[[0.1,0.8,0.9,0.8,0.1,0.8,0.9,0.8,0.1],\n",
    "                                                [0.9,0.2,0.1,0.2,0.9,0.2,0.1,0.2,0.9]],\n",
    "                                                evidence=['dimension1','dimension2'], evidence_card=[3,3])\n",
    "cpd_letter_spacing_sim = TabularCPD('letter_spacing_sim',2,[[0.1,0.8,0.9,0.8,0.1,0.8,0.9,0.8,0.1],\n",
    "                                                [0.9,0.2,0.1,0.2,0.9,0.2,0.1,0.2,0.9]],\n",
    "                                                evidence=['letter_spacing1','letter_spacing2'], evidence_card=[3,3])\n",
    "cpd_size_sim = TabularCPD('size_sim',2,[[0.6,0.3,0.3,0.1,0.8,0.7,0.7,0.3,0.9,0.8,0.7,0.4,0.7,0.6,0.6,0.3,0.6,0.3,0.3,0.1,0.8,0.4,0.4,0.85,0.9,0.8,0.8,0.3,0.8,0.4,0.4,0.85,0.6,0.3,0.3,0.1],\n",
    "                                        [0.4,0.7,0.7,0.9,0.2,0.3,0.3,0.7,0.1,0.2,0.3,0.6,0.3,0.4,0.4,0.7,0.4,0.7,0.7,0.9,0.2,0.6,0.6,0.15,0.1,0.2,0.2,0.7,0.2,0.6,0.6,0.15,0.4,0.7,0.7,0.9]],\n",
    "                                        evidence=['size1','size2','dimension_sim','letter_spacing_sim'], evidence_card=[3,3,2,2])\n",
    "cpd_constancy_sim = TabularCPD('constancy_sim',2,[[0.9,0.1,0.9,0.6,0.9,0.6,0.7,0.1],\n",
    "                                        [0.1,0.9,0.1,0.4,0.1,0.4,0.3,0.9]],\n",
    "                                        evidence=['constancy1','constancy2','size_sim'], evidence_card=[2,2,2])\n",
    "cpd_word_formation_sim = TabularCPD('word_formation_sim',2,[[0.9,0.1,0.9,0.7,0.9,0.7,0.9,0.1],\n",
    "                                        [0.1,0.9,0.1,0.3,0.1,0.3,0.1,0.9]],\n",
    "                                        evidence=['word_formation1','word_formation2','constancy_sim'], evidence_card=[2,2,2])\n",
    "cpd_formation_n_sim = TabularCPD('formation_n_sim',2,[[0.7,0.1,0.9,0.4,0.9,0.4,0.6,0.1],\n",
    "                                        [0.3,0.9,0.1,0.6,0.1,0.6,0.4,0.9]],\n",
    "                                        evidence=['formation_n1','formation_n2','word_formation_sim'], evidence_card=[2,2,2])\n",
    "cpd_entry_stroke_a_sim = TabularCPD('entry_stroke_a_sim',2,[[0.1,0.9,0.9,0.1],\n",
    "                                                            [0.9,0.1,0.1,0.9]],\n",
    "                                        evidence=['entry_stroke_a1','entry_stroke_a2'], evidence_card=[2,2])\n",
    "\n",
    "combined_model.add_cpds(cpd_pen_pressure1,\n",
    "                        cpd_pen_pressure2,\n",
    "                        cpd_is_pen_pressure_sim,\n",
    "                        cpd_slantness1,\n",
    "                        cpd_slantness2,\n",
    "                        cpd_is_slantness_sim,\n",
    "                        cpd_tilt1,\n",
    "                        cpd_tilt2,\n",
    "                        cpd_is_tilt_sim,\n",
    "                        cpd_staff_of_a1,\n",
    "                        cpd_staff_of_a2,\n",
    "                        cpd_is_staff_of_a_sim,\n",
    "                        cpd_staff_of_d1,\n",
    "                        cpd_staff_of_d2,\n",
    "                        cpd_is_staff_of_d_sim,\n",
    "                        cpd_exit_stroke_d1,\n",
    "                        cpd_exit_stroke_d2,\n",
    "                        cpd_is_exit_stroke_d_sim,\n",
    "                        cpd_is_lowercase1,\n",
    "                        cpd_is_lowercase2,\n",
    "                        cpd_is_lowercase_sim,\n",
    "                        cpd_is_continuous1,\n",
    "                        cpd_is_continuous2,\n",
    "                        cpd_is_continuous_sim,\n",
    "                        cpd_dimension1,\n",
    "                        cpd_dimension2,\n",
    "                        cpd_dimension_sim,\n",
    "                        cpd_letter_spacing1,\n",
    "                        cpd_letter_spacing2,\n",
    "                        cpd_letter_spacing_sim,\n",
    "                        cpd_size1,\n",
    "                        cpd_size2,\n",
    "                        cpd_size_sim,\n",
    "                        cpd_constancy1,\n",
    "                        cpd_constancy2,\n",
    "                        cpd_constancy_sim,\n",
    "                        cpd_word_formation1,\n",
    "                        cpd_word_formation2,\n",
    "                        cpd_word_formation_sim,\n",
    "                        cpd_formation_n1,\n",
    "                        cpd_formation_n2,\n",
    "                        cpd_formation_n_sim,\n",
    "                        cpd_entry_stroke_a1,\n",
    "                        cpd_entry_stroke_a2,\n",
    "                        cpd_entry_stroke_a_sim\n",
    "                       )\n",
    "combined_model.check_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = BIFWriter(combined_model)\n",
    "model_data.write_bif(filename='weights/pgmModel.bif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mle = VariableElimination(combined_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]pen_pressure\n",
      "[0 1 2]letter_spacing\n",
      "[0 1 2]size\n",
      "[0 1 2]dimension\n",
      "[0 1]is_lowercase\n",
      "[0 1]is_continuous\n",
      "[0 1 2 3]slantness\n",
      "[0 1]tilt\n",
      "[0 1]entry_stroke_a\n",
      "[0 1 2 3]staff_of_a\n",
      "[0 1]formation_n\n",
      "[0 1 2]staff_of_d\n",
      "[0 1 2 3]exit_stroke_d\n",
      "[0 1]word_formation\n",
      "[0 1]constancy\n"
     ]
    }
   ],
   "source": [
    "for idx,columns in enumerate(feature_data.columns):\n",
    "    if idx != 0:\n",
    "        print(str(np.unique(feature_data[columns]))+columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning the weights in Structured CPD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Used c4.8xlarge to run inferences for 3hours, around 10-11 iterations per second\n",
    "On my pc i3 estimate was 5 hours\n",
    "Using instances > c4.8xlarge doesn't affect much in inference\n",
    "Pgmpy Why are you not parallelizable?\n",
    "'''\n",
    "\n",
    "'''\n",
    "simFeatures = [[] for _ in range(100)]\n",
    "var = {'is_pen_pressure_sim',\n",
    "       'is_slantness_sim',\n",
    "       'is_tilt_sim',\n",
    "       'is_staff_of_a_sim',\n",
    "       'is_staff_of_d_sim',\n",
    "       'entry_stroke_a_sim',\n",
    "       'is_exit_stroke_d_sim',\n",
    "      'is_lowercase_sim',\n",
    "      'is_continuous_sim',\n",
    "      'dimension_sim',\n",
    "      'letter_spacing_sim',\n",
    "       'size_sim',\n",
    "       'constancy_sim',\n",
    "       'word_formation_sim',\n",
    "       'formation_n_sim'\n",
    "      }\n",
    "evidence_labels = trainData.columns[3:]\n",
    "for idx in tqdm(range(100)):\n",
    "    inf = mle.query(variables=var,evidence=dict(zip(evidence_labels,trainData.iloc[idx,3:].tolist())))\n",
    "    for simfeature in var:\n",
    "        simFeatures[idx].append(np.argmax(inf[simfeature].values))\n",
    "\n",
    "simDf = pd.DataFrame(data=simFeatures,columns=var)\n",
    "\n",
    "simDf = pd.concat([simDf,trainData.label],axis=1)\n",
    "\n",
    "simDf.to_csv(\"./sigTrainData.csv\")\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = {'is_pen_pressure_sim',\n",
    "       'is_slantness_sim',\n",
    "       'is_tilt_sim',\n",
    "       'is_staff_of_a_sim',\n",
    "       'is_staff_of_d_sim',\n",
    "       'entry_stroke_a_sim',\n",
    "       'is_exit_stroke_d_sim',\n",
    "      'is_lowercase_sim',\n",
    "      'is_continuous_sim',\n",
    "      'dimension_sim',\n",
    "      'letter_spacing_sim',\n",
    "       'size_sim',\n",
    "       'constancy_sim',\n",
    "       'word_formation_sim',\n",
    "       'formation_n_sim'\n",
    "      }\n",
    "evidence_labels = trainData.columns[3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'simDf = pd.read_csv(\"./sigTrainData.csv\")\\nsimDf = simDf.iloc[0:,1:]\\nsimDf.head()'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''simDf = pd.read_csv(\"./sigTrainData.csv\")\n",
    "simDf = simDf.iloc[0:,1:]\n",
    "simDf.head()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''xTrain = simDf.iloc[0:,0:15].values.tolist()\n",
    "xTrain = np.array(xTrain)\n",
    "yTrain = np.array(simDf.iloc[0:,15:].values.tolist())\n",
    "yTrain = yTrain.ravel()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7442358016832874 0.7581095441364529 0.7511086128286804\n"
     ]
    }
   ],
   "source": [
    "'''model = LogisticRegressionCV(cv=100, random_state=0,\n",
    "                            fit_intercept=True,max_iter=10000).fit(xTrain, yTrain)\n",
    "\n",
    "pred = model.predict(xTrain)\n",
    "\n",
    "precision,recall,f1,_ = precision_recall_fscore_support(yTrain, pred, average='binary')\n",
    "print(precision,recall,f1)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''weights = model.coef_'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''with open('weights/sigmoid_cpd_weight', 'wb') as fp:\n",
    "    pickle.dump(weights, fp)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''with open('weights/sigmoid_cpd_bias', 'wb') as fp:\n",
    "    pickle.dump(b, fp)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deterministic_sigmoid_node(biasNodeAccumulatorNode):\n",
    "    return 1/(1+np.exp(biasNodeAccumulatorNode*-1))\n",
    "\n",
    "def deterministic_verification_node(nodeOutputs,weights,b):\n",
    "    weightFeaturesAccumulatorNode = np.dot(weights,nodeOutputs)\n",
    "    biasNodeAccumulatorNode = b[0]+weightFeaturesAccumulatorNode[0]\n",
    "    return deterministic_sigmoid_node(biasNodeAccumulatorNode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7221 [00:00<?, ?it/s]/home/pratik/anaconda3/envs/expKerasPgm/lib/python3.7/site-packages/pgmpy/factors/discrete/DiscreteFactor.py:586: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  phi.values = phi.values[slice_]\n",
      "/home/pratik/anaconda3/envs/expKerasPgm/lib/python3.7/site-packages/pgmpy/factors/discrete/DiscreteFactor.py:598: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  phi1.values = phi1.values[slice_]\n",
      "100%|██████████| 7221/7221 [21:07<00:00,  5.84it/s]\n"
     ]
    }
   ],
   "source": [
    "simFeatures_val = [[] for _ in range(len(val_data))]\n",
    "evidence_labels = val_data.columns[3:]\n",
    "for idx in tqdm(range(len(val_data))):\n",
    "    inf = mle.query(variables=var,evidence=dict(zip(evidence_labels,val_data.iloc[idx,3:].tolist())))\n",
    "    for simfeature in var:\n",
    "        simFeatures_val[idx].append(np.argmax(inf[simfeature].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"deterministicNodePred = []\\nfor nodeOutputs in simFeatures_val:\\n    deterministicNodePred.append(deterministic_verification_node(nodeOutputs,weights,b))\\n\\nprecision,recall,f1,_ = precision_recall_fscore_support(list(val_data.label), np.round(deterministicNodePred), average='binary')\\nprint(precision,recall,f1)\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''deterministicNodePred = []\n",
    "for nodeOutputs in simFeatures_val:\n",
    "    deterministicNodePred.append(deterministic_verification_node(nodeOutputs,weights,b))\n",
    "\n",
    "precision,recall,f1,_ = precision_recall_fscore_support(list(val_data.label), np.round(deterministicNodePred), average='binary')\n",
    "print(precision,recall,f1)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy based Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropyDict={}\n",
    "for feature in trainData.columns[3:]:\n",
    "    feature_states = np.unique(trainData[feature])\n",
    "    temp = []\n",
    "    for state in feature_states:\n",
    "        query = str(feature+\"==\"+str(state))\n",
    "        prob = len(trainData.query(query))/len(trainData[feature])\n",
    "        temp.append(prob)\n",
    "    entropyDict[feature] = temp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 127273/127273 [01:40<00:00, 1272.72it/s]\n"
     ]
    }
   ],
   "source": [
    "entropyRows_pos = []\n",
    "entropyRows_neg = []\n",
    "for idx_r in tqdm(range(len(trainData))):\n",
    "    entropy = 0\n",
    "    for idx_c,cols in enumerate(trainData.columns[3:]):\n",
    "        prob = entropyDict[cols][int(trainData.iloc[idx_r,idx_c+3])]\n",
    "        entropy -= prob * np.log2(prob)\n",
    "    if(trainData.iloc[idx_r,2] == 1):\n",
    "        entropyRows_pos.append(entropy)\n",
    "    if(trainData.iloc[idx_r,2] == 0):\n",
    "        entropyRows_neg.append(entropy)\n",
    "entropyRows_pos = np.array(entropyRows_pos)\n",
    "entropyRows_neg = np.array(entropyRows_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "top100_pos = entropyRows_pos.argsort()[-500:][::-1]\n",
    "top100_neg = entropyRows_neg.argsort()[-100:][::-1]\n",
    "entropyData = np.append(top100_pos,top100_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [01:48<00:00,  5.89it/s]\n"
     ]
    }
   ],
   "source": [
    "entropyTrain = [[] for _ in range(len(entropyData))]\n",
    "entropyLabels = []\n",
    "for i,idx in enumerate(tqdm(entropyData)):\n",
    "    inf = mle.query(variables=var,evidence=dict(zip(evidence_labels,trainData.iloc[idx,3:].tolist())))\n",
    "    entropyLabels.append(trainData.iloc[idx,32])\n",
    "    for simfeature in var:\n",
    "        entropyTrain[i].append(np.argmax(inf[simfeature].values))\n",
    "entropyLabels = np.array(entropyLabels)\n",
    "entropyTrain = np.array(entropyTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6678082191780822 0.9848484848484849 0.7959183673469388\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegressionCV(cv=10,random_state=0,\n",
    "                            fit_intercept=True,max_iter=500,penalty='l2').fit(entropyTrain, entropyLabels)\n",
    "\n",
    "pred = model.predict(entropyTrain)\n",
    "\n",
    "precision,recall,f1,_ = precision_recall_fscore_support(entropyLabels, pred, average='binary')\n",
    "print(precision,recall,f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.coef_\n",
    "b = model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../weights/sigmoid_cpd_weight_entropy_unseen', 'wb') as fp:\n",
    "    pickle.dump(weights, fp)\n",
    "with open('../weights/sigmoid_cpd_bias_entropy_unseen', 'wb') as fp:\n",
    "    pickle.dump(b, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48645179691956647 0.9584152851924698 0.6453504871819128\n"
     ]
    }
   ],
   "source": [
    "deterministicNodePred = []\n",
    "for nodeOutputs in simFeatures_val:\n",
    "    deterministicNodePred.append(deterministic_verification_node(nodeOutputs,weights,b))\n",
    "    \n",
    "precision,recall,f1,_ = precision_recall_fscore_support(list(val_data.label), np.round(deterministicNodePred), average='binary')\n",
    "print(precision,recall,f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Entropy based approach needs more research, there is an overfitting due to only high entropy values provided. Maybe\n",
    "a different classifier could be used but then it wouldn't be sigmoid cpd. Regularization doesn't help.\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
